{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import socket\n",
    "import struct\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "import spinup\n",
    "import core_FPGA as core\n",
    "\n",
    "from utils import weights_creation, create_input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPISODES = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_sizes = [core.N_HIDDEN_1, core.N_HIDDEN_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 100000  # 100 kHz\n",
    "TIME_STEP = 1 / SAMPLE_RATE\n",
    "\n",
    "MAX_VAL = 32767\n",
    "MAX_V_DAC = 5\n",
    "MAX_V_ADC = 25\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import struct\n",
    "import socket\n",
    "\n",
    "def request_and_receive_data(server_ip, server_port, experiment_folder):\n",
    "  # Set up the socket connection\n",
    "  client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "  client.settimeout(60 * 60)  # Set the timeout for blocking socket operations, 1h\n",
    "  client.connect((server_ip, server_port))\n",
    "\n",
    "  # Instantiate the digital twin \n",
    "  model = core.Digital_twin(core.N_INPUT, 1, hidden_sizes, device).to(device)\n",
    "\n",
    "  # Have to run inference once - create random data\n",
    "  in_float = create_input_tensor(core.N_INPUT, device)\n",
    "  \n",
    "  model.eval()\n",
    "  \n",
    "  out_brevitas = model(in_float)\n",
    "\n",
    "  scale = float(out_brevitas.scale[0, 0])  \n",
    "\n",
    "  for ep in range(N_EPISODES):\n",
    "      weights_list, random_numbers = weights_creation(model, scale, ep)\n",
    "      \n",
    "      print(f\"Ready for Run {ep}/{N_EPISODES-1}:\")\n",
    "      \n",
    "      # Serialize and send the weights list; here, each weight is packed as a single byte, reflecting the int8 data type.\n",
    "      packed_weights = b''.join(struct.pack('<b', weight) for weight in weights_list)\n",
    "      client.send(packed_weights) # Send it all \n",
    "\n",
    "      print(\"Done streaming weights.\")\n",
    "      \n",
    "      obs, actions = core.receive_data_episode(client, core.N_STEPS * core.N_INPUT, core.N_STEPS)\n",
    "\n",
    "      # Save the data\n",
    "      save_data(obs, actions, ep, experiment_folder)\n",
    "\n",
    "  # Save the model\n",
    "  save_model(model, experiment_folder)\n",
    "\n",
    "  # Close the socket connection\n",
    "  client.close()\n",
    "\n",
    "def save_data(obs, actions, episode, experiment_folder):\n",
    "  data_folder = os.path.join(experiment_folder, 'data')\n",
    "  os.makedirs(data_folder, exist_ok=True)\n",
    "  \n",
    "  np.save(os.path.join(data_folder, f'obs_episode_{episode}.npy'), obs)\n",
    "  np.save(os.path.join(data_folder, f'actions_episode_{episode}.npy'), actions)\n",
    "\n",
    "def save_model(model, experiment_folder):\n",
    "  model_folder = os.path.join(experiment_folder, 'model')\n",
    "  os.makedirs(model_folder, exist_ok=True)\n",
    "  \n",
    "  torch.save(model.state_dict(), os.path.join(model_folder, 'model.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\Anaconda3\\envs\\brevitas_env\\Lib\\site-packages\\torch\\_tensor.py:1413: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\c10/core/TensorImpl.h:1928.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 0/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 1/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 2/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 3/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 4/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 5/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 6/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 7/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 8/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 9/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 10/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 11/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 12/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 13/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 14/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 15/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 16/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 17/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 18/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n",
      "Sent Scale: 3.9910099964429735e-13\n",
      "Ready for Run 19/19:\n",
      "Done streaming weights.\n",
      "Number of received actions: 324\n"
     ]
    }
   ],
   "source": [
    "experiment_folder = 'new_no_over_scope_data'\n",
    "server_ip = '192.168.1.10'\n",
    "server_port = 7\n",
    "\n",
    "errors = request_and_receive_data(server_ip, server_port, experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(experiment_folder):\n",
    "  model_path = os.path.join(experiment_folder, 'model', 'model.pth')\n",
    "  \n",
    "  # Instantiate the model with the same architecture as when it was saved\n",
    "  model = core.Digital_twin(core.N_INPUT, 1, hidden_sizes, device).to(device)\n",
    "  \n",
    "  # Load the state dict\n",
    "  state_dict = torch.load(model_path, map_location=device)\n",
    "  \n",
    "  # Try to load the state dict, ignoring mismatched keys\n",
    "  model.load_state_dict(state_dict, strict=False)\n",
    "  \n",
    "  model.eval()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new_no_over_scope_data'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_21924\\1678710607.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = load_model(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
